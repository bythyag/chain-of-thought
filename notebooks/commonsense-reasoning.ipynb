{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc318f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from openai import OpenAI\n",
    "from google.genai import types\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import requests\n",
    "import gzip\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure Gemini\n",
    "from google import genai\n",
    "api_key=os.getenv('GEMINI_API')\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04cd8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since commonsense_qa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/thyag/.cache/huggingface/datasets/commonsense_qa/default/0.0.0/94630fe30dad47192a8546eb75f094926d47e155 (last modified on Sat Jul 12 21:50:08 2025).\n",
      "Using the latest cached version of the dataset since metaeval/strategy-qa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/thyag/.cache/huggingface/datasets/metaeval___strategy-qa/default/0.0.0/f0e71799fcfdee70d1618765168904370cfec189 (last modified on Sat Jul 12 21:50:17 2025).\n",
      "Using the latest cached version of the dataset since chiayewken/saycan couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/thyag/.cache/huggingface/datasets/chiayewken___saycan/default/0.0.0/4cc33f0ebe8b27733035e097b7bd4976fc1a786d (last modified on Sat Jul 12 22:01:56 2025).\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "# 1. CSQA – CommonsenseQA (train/dev/test splits)\n",
    "csqa = load_dataset(\"commonsense_qa\")          \n",
    "\n",
    "# 2. StrategyQA – yes/no questions requiring multi-hop reasoning\n",
    "strategyqa = load_dataset(\"metaeval/strategy-qa\")\n",
    "\n",
    "# 3. Date Understanding\n",
    "date_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\n",
    "date_response = requests.get(date_url)\n",
    "date = json.loads(date_response.text)\n",
    "\n",
    "# 4. Sports Understanding – sports-related questions\n",
    "sports_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/sports_understanding/task.json\"\n",
    "sports_response = requests.get(sports_url)\n",
    "sports = json.loads(sports_response.text)\n",
    "\n",
    "# 5. SayCan – Language→robot-action mapping (Google Robotics, 2022)\n",
    "saycan = load_dataset(\"chiayewken/saycan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc4d5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template functions for commonsense reasoning tasks\n",
    "\n",
    "def csqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a CommonsenseQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question', 'choices', and 'answerKey'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    labels = [label.lower() for label in example['choices']['label']]\n",
    "    texts = example['choices']['text']\n",
    "    choices_str = \" \".join([f\"({label}) {text}\" for label, text in zip(labels, texts)])\n",
    "    question_template = f\"{example['question']} Answer Choices: {choices_str}\"\n",
    "    answer = f\"The answer is ({example['answerKey'].lower()})\"\n",
    "    return question_template, answer\n",
    "\n",
    "def strategyqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a StrategyQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'answer'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "    answer = \"yes\" if example['answer'] else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def date_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a date understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['input']\n",
    "    correct_answer = str(next((k for k, v in example['target_scores'].items() if v == 1), None))\n",
    "    answer = f\"The answer is {correct_answer}\"\n",
    "    return question, answer\n",
    "\n",
    "def sports_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a sports understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'input' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = f'Is the following sentence plausible? \"{example[\"input\"]}\"'\n",
    "    answer = \"yes\" if example[\"target_scores\"].get(\"plausible\", 0) == 1 else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def saycan_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a SayCan example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'instruction' and 'plan'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['INPUT']\n",
    "    answer = f\"The answer is {example['OUTPUT']}\"\n",
    "    return question, answer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4d7f647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['INPUT', 'OUTPUT'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saycan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2b586f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- csqa ---\n",
      "Question: Where would you find a hamburger? Answer Choices: (a) fast food restaurant (b) pizza (c) kitchen (d) grocery store (e) farms\n",
      "Answer  : The answer is (a)\n",
      "\n",
      "--- strategyqa ---\n",
      "Question: Would a person with a high fever typically sweat?\n",
      "Answer  : The answer is yes\n",
      "\n",
      "--- date ---\n",
      "Question: What day of the week was 2020-02-02?\n",
      "Answer  : The answer is Sunday\n",
      "\n",
      "--- sports ---\n",
      "Question: Is the following sentence plausible? \"The quarterback threw a touchdown pass.\"\n",
      "Answer  : The answer is yes\n",
      "\n",
      "--- saycan ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'INPUT'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(q, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m q.strip()\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m a.strip()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mtest_smoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtest_smoke\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset, tpl \u001b[38;5;129;01min\u001b[39;00m templates.items():\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     q, a = \u001b[43mtpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAL_EXAMPLES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, q)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer  :\u001b[39m\u001b[33m\"\u001b[39m, a)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36msaycan_template\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msaycan_template\u001b[39m(example: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    Converts a SayCan example dict to the question template.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m \u001b[33;03m        tuple: (question string, answer string)\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     question = \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mINPUT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     66\u001b[39m     answer = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe answer is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mOUTPUT\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m question, answer\n",
      "\u001b[31mKeyError\u001b[39m: 'INPUT'"
     ]
    }
   ],
   "source": [
    "# quick smoke test for templates\n",
    "\n",
    "REAL_EXAMPLES = {\n",
    "    \"csqa\": {\n",
    "        \"question\": \"Where would you find a hamburger?\",\n",
    "        \"choices\": {\n",
    "            \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "            \"text\": [\"fast food restaurant\", \"pizza\", \"kitchen\", \"grocery store\", \"farms\"]\n",
    "        },\n",
    "        \"answerKey\": \"A\"\n",
    "    },\n",
    "    \"strategyqa\": {\n",
    "        \"question\": \"Would a person with a high fever typically sweat?\",\n",
    "        \"answer\": True\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"input\": \"What day of the week was 2020-02-02?\",\n",
    "        \"target_scores\": {\"Sunday\": 1, \"Monday\": 0, \"Tuesday\": 0}\n",
    "    },\n",
    "    \"sports\": {\n",
    "        \"input\": \"The quarterback threw a touchdown pass.\",\n",
    "        \"target_scores\": {\"plausible\": 1}\n",
    "    },\n",
    "    \"saycan\": {\n",
    "        \"instruction\": \"Bring me a cold drink.\",\n",
    "        \"plan\": \"open fridge, pick drink, close fridge, bring drink\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def test_smoke():\n",
    "    \"\"\"Sanity check with the hand-crafted examples above.\"\"\"\n",
    "    templates = {\n",
    "        \"csqa\": csqa_template,\n",
    "        \"strategyqa\": strategyqa_template,\n",
    "        \"date\": date_template,\n",
    "        \"sports\": sports_template,\n",
    "        \"saycan\": saycan_template,\n",
    "    }\n",
    "\n",
    "    for dataset, tpl in templates.items():\n",
    "        print(f\"\\n--- {dataset} ---\")\n",
    "        q, a = tpl(REAL_EXAMPLES[dataset])\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer  :\", a)\n",
    "        # basic assertions\n",
    "        assert isinstance(q, str) and q.strip()\n",
    "        assert isinstance(a, str) and a.strip()\n",
    "\n",
    "test_smoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "51c72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for CSQA\n",
    "\n",
    "CSQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\n",
    "calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.\n",
    "So the answer is (e).\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
    "A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\n",
    "Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b)\n",
    "hen house (c) natural habitat (d) storybook\n",
    "A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the\n",
    "answer is (b).\n",
    "Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\n",
    "(b) race track (c) desert (d) apartment (e) roadblock\n",
    "A: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of\n",
    "people. So the answer is (a).\n",
    "Q: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\n",
    "market (d) fruit basket (e) fruit market\n",
    "A: The answer should be the place where grocery items are placed before checking out. Of the above choices,\n",
    "grocery cart makes the most sense for holding grocery items. So the answer is (b).\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A: The answer must be something that used to do what Google Maps and GPS services do, which is to give\n",
    "directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\n",
    "Q: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\n",
    "anguish (c) bitterness (d) tears (e) sadness\n",
    "A: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above\n",
    "choices, the closest feeling is bitterness. So the answer is (c).\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b798037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for StrategyQA\n",
    "\n",
    "StrategyQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: Do hamsters provide food for any animals?\n",
    "A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So\n",
    "the answer is yes.\n",
    "Q: Could Brooke Shields succeed at University of Pennsylvania?\n",
    "A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\n",
    "University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the\n",
    "answer is yes.\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic\n",
    "number squared is less than 5. So the answer is no.\n",
    "Q: Yes or no: Is it common to see frost during some college commencements?\n",
    "A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\n",
    "there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\n",
    "Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n",
    "A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\n",
    "months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\n",
    "Q: Yes or no: Would a pear sink in water?\n",
    "A: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus,\n",
    "a pear would float. So the answer is no.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "79cd74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for date understanding \n",
    "\n",
    "Date_PROMPT = \"\"\"\n",
    "\n",
    "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today\n",
    "is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.\n",
    "Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in\n",
    "MM/DD/YYYY?\n",
    "A: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first monday, would be six\n",
    "days later. So today is 01/07/2019. So the answer is 01/07/2019.\n",
    "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\n",
    "days ago in MM/DD/YYYY?\n",
    "A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the\n",
    "answer is 05/23/1943.\n",
    "Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is\n",
    "04/20/1969.\n",
    "Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\n",
    "later in MM/DD/YYYY?\n",
    "A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\n",
    "Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\n",
    "yesterday in MM/DD/YYYY?\n",
    "A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so\n",
    "today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a09ebddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for sports understanding\n",
    "\n",
    "Sports_PROMPT = \"\"\"\n",
    "\n",
    "Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\n",
    "A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\n",
    "A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”\n",
    "A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer\n",
    "is no.\n",
    "Q: Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\n",
    "A: Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Jamel Murray was perfect from the line.”\n",
    "A: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Sam Darnold passed the puck.”\n",
    "A: Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A: Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\n",
    "A: Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. So the answer is yes.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c971580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for saycan \n",
    "\n",
    "SayCan_PROMPT = \"\"\"\n",
    "\n",
    "Locations = [counter, table, user, trash, bowl].\n",
    "Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\n",
    "grapefruit soda, pepsi, redbull, energy bar, sponge, water].\n",
    "The robot can pick up items with pick(object) and put down items with put(object) as well as find objects or\n",
    "locations with find(). The robot can only understand the explicit locations and objects listed.\n",
    "Human: I’m hungry, can you bring me some chips.\n",
    "Explanation: The user is hungry and has asked for chips. There are several types of chips available, I will bring\n",
    "the user the kettle chips.\n",
    "Plan: 1. find(kettle chips), 2. pick(kettle chips), 3. find(user), 4. put(kettle chips), 5. done().\n",
    "Human: How would you move the grapefruit drink from the table to the counter?\n",
    "Explanation: The user has asked me to move the grapefruit drink to the counter.\n",
    "Plan: 1. find(grapefruit soda), 2. pick(grapefruit soda), 3. find(counter), 4. put(grapefruit soda), 5. done().\n",
    "Human: How would you bring me some snacks?\n",
    "Explanation: The user has asked for snacks, I will choose two items and bring them. I will bring jalapeno chips\n",
    "and an apple.\n",
    "Plan: 1. find(jalapeno chips), 2. pick(jalapeno chips), 3. find(user), 4. put(jalapeno chips), 5. find(apple), 6.\n",
    "pick(apple), 7. find(user), 8. put(apple), 9. done().\n",
    "Human: How would you bring me something to eat that isn’t a fruit?\n",
    "Explanation: The user has asked for a food that isn’t a fruit, I will bring an energy bar to them.\n",
    "Plan: 1. find(energy bar), 2. pick(energy bar), 3. find(user), 4. put(energy bar), 5. done().\n",
    "Human: How would you put the rice chips in the bowl and then move the tea to the table?\n",
    "Explanation: The user has asked me to do two tasks, I will do one and then the other.\n",
    "Plan: 1. find(rice chips), 2. pick(rice chips), 3. find(bowl), 4. put(rice chips), 5. find(tea), 6. pick(tea), 7.\n",
    "find(table), 8. put(tea), 9. done().\n",
    "Human: How would you throw away a redbull?\n",
    "Explanation: The user has asked me to throw away the redbull, I will move it to the trash.\n",
    "Plan: 1. find(redbull), 2. pick(redbull), 3. find(trash), 4. put(redbull), 5. done().\n",
    "Human: Bring me a drink.\n",
    "Explanation: The user has asked for a drink and there are many options. I will bring them a water.\n",
    "Plan: 1. find(water), 2. pick(water), 3. find(user), 4. put(water), 5. done().\n",
    "\n",
    "\n",
    "Human: {Question}\n",
    "Explanation:\n",
    "Plan:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62ca9ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['qid', 'term', 'description', 'question', 'answer', 'facts', 'decomposition'],\n",
       "        num_rows: 2290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategyqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18775a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['INPUT', 'OUTPUT'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saycan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d81d3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up template functions\n",
    "template_functions = {\n",
    "    #\"csqa\": csqa_template,\n",
    "    \"strategyqa\": strategyqa_template,\n",
    "    \"date\": date_template,\n",
    "    \"sports\": sports_template,\n",
    "    \"saycan\": saycan_template\n",
    "}\n",
    "\n",
    "# Set up prompt templates\n",
    "prompt_templates = {\n",
    "    #\"csqa\": CSQA_PROMPT,\n",
    "    \"strategyqa\": StrategyQA_PROMPT,\n",
    "    \"date\": Date_PROMPT,\n",
    "    \"sports\": Sports_PROMPT,\n",
    "    \"saycan\": SayCan_PROMPT\n",
    "}\n",
    "\n",
    "# Set up datasets\n",
    "datasets = {\n",
    "    #\"csqa\": csqa,\n",
    "    \"strategyqa\": strategyqa,\n",
    "    \"date\": date,\n",
    "    \"sports\": sports,\n",
    "    \"saycan\": saycan\n",
    "}\n",
    "\n",
    "# Define models and system prompts\n",
    "openai_models = [] #\"gpt-4.1-nano\", \"gpt-4o-mini-2024-07-18\"\n",
    "gemini_models = [ \"gemini-1.5-flash\"]\n",
    "system_prompts = {\n",
    "    \"few_shot\": \"You are a helpful assistant. Carefully follow the reasoning and answer format shown in the few-shot examples provided by the user. Use the same step-by-step explanation style and answer format.\",\n",
    "    \"zero_shot\": \"You are a helpful assistant. First, provide a detailed answer to the question. Then, on a new line, clearly state your final answer in the format: The answer is {answer}. Only include the final answer in that line, without any explanation.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5034647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key found\n",
      "✅ Gemini API key found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure Gemini\n",
    "from google import genai\n",
    "client = genai.Client(api_key=\"xx\")\n",
    "\n",
    "\n",
    "# Add this to check if API keys loaded successfully\n",
    "def check_api_keys():\n",
    "    # Check keys\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"✅ OpenAI API key found\")\n",
    "    else:\n",
    "        print(\"❌ OpenAI API key missing\")\n",
    "        \n",
    "    if os.getenv('GEMINI_API'):\n",
    "        print(\"✅ Gemini API key found\")\n",
    "    else:\n",
    "        print(\"❌ Gemini API key missing\")\n",
    "\n",
    "check_api_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13a00e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "def test_models(openai_models: List[str], gemini_models: List[str]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Test if all models are accessible and working properly.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize clients properly\n",
    "    openai_client = OpenAI()\n",
    "    \n",
    "    # Simple test prompt\n",
    "    test_prompt = \"Complete this sentence: The sky is\"\n",
    "    \n",
    "    print(\"Testing OpenAI models...\")\n",
    "    for model in openai_models:\n",
    "        print(f\"  Testing {model}...\", end=\"\", flush=True)\n",
    "        try:\n",
    "            response = openai_client.responses.create(\n",
    "                model=model,\n",
    "                instructions=\"You are a helpful assistant.\",\n",
    "                input=test_prompt,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            print(f\" ✓ Success: {response.output_text[:30]}...\")\n",
    "            results[model] = True\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Failed: {str(e)[:100]}...\")\n",
    "            results[model] = False\n",
    "    \n",
    "    print(\"\\nTesting Gemini models...\")\n",
    "    for model in gemini_models:\n",
    "        print(f\"  Testing {model}...\", end=\"\", flush=True)\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=test_prompt,\n",
    "                config=types.GenerateContentConfig(temperature=0)\n",
    "            )\n",
    "            print(f\" ✓ Success: {response.text[:30]}...\")\n",
    "            results[model] = True\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Failed: {str(e)[:100]}...\")\n",
    "            results[model] = False\n",
    "        \n",
    "        # Add small delay between tests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        datasets: Dict[str, any],\n",
    "        template_functions: Dict[str, Callable],\n",
    "        prompt_templates: Dict[str, str],\n",
    "        openai_models: List[str],\n",
    "        gemini_models: List[str],\n",
    "        system_prompts: Dict[str, str],\n",
    "        output_dir: str = \"results\",\n",
    "        sample_size: int = 20\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the benchmark runner for commonsense reasoning tasks.\n",
    "        \n",
    "        Args:\n",
    "            datasets: Dictionary mapping dataset names to dataset objects\n",
    "            template_functions: Dictionary mapping dataset names to template functions\n",
    "            prompt_templates: Dictionary mapping dataset names to prompt templates\n",
    "            openai_models: List of OpenAI model names to test\n",
    "            gemini_models: List of Gemini model names to test\n",
    "            system_prompts: Dictionary with system prompts for few_shot and zero_shot modes\n",
    "            output_dir: Directory to save results\n",
    "            sample_size: Number of examples to run per dataset\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.template_functions = template_functions\n",
    "        self.prompt_templates = prompt_templates\n",
    "        self.openai_models = openai_models\n",
    "        self.gemini_models = gemini_models\n",
    "        self.system_prompts = system_prompts\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        # Initialize clients\n",
    "        self.openai_client = OpenAI()\n",
    "        \n",
    "        # Get Gemini API key from environment\n",
    "        # We don't need to initialize Gemini client here anymore\n",
    "        # since we're using the globally configured genai module\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def run_openai(self, prompt_type: str, dataset_name: str, model_name: str, examples: List[Dict]):\n",
    "        \"\"\"\n",
    "        Run benchmarks with OpenAI models using their specific API format\n",
    "        \n",
    "        Args:\n",
    "            prompt_type: \"few_shot\" or \"zero_shot\"\n",
    "            dataset_name: Name of the dataset\n",
    "            model_name: Name of the model\n",
    "            examples: List of processed examples with questions and expected answers\n",
    "        \"\"\"\n",
    "        chat_responses = []\n",
    "        \n",
    "        for i in tqdm(range(min(self.sample_size, len(examples))), desc=f\"OpenAI {model_name} on {dataset_name}\"):\n",
    "            question, original_answer = examples[i]\n",
    "            \n",
    "            if prompt_type == \"few_shot\":\n",
    "                prompt = self.prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "                instructions = self.system_prompts[\"few_shot\"]\n",
    "            else:  # zero_shot\n",
    "                prompt = question\n",
    "                instructions = self.system_prompts[\"zero_shot\"]\n",
    "            \n",
    "            try:\n",
    "                response = self.openai_client.responses.create(\n",
    "                    model=model_name,\n",
    "                    instructions=instructions,\n",
    "                    input=prompt,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "                model_answer = response.output_text\n",
    "            except Exception as e:\n",
    "                print(f\"Error with OpenAI model {model_name}: {e}\")\n",
    "                model_answer = f\"ERROR: {str(e)}\"\n",
    "            \n",
    "            chat_responses.append({\n",
    "                \"question\": question,\n",
    "                \"original_answer\": original_answer,\n",
    "                \"answer_text\": model_answer\n",
    "            })\n",
    "        \n",
    "        # Save results\n",
    "        output_filename = os.path.join(\n",
    "            self.output_dir,\n",
    "            f\"{dataset_name}-{prompt_type}-{model_name.replace('.', '-')}.json\"\n",
    "        )\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chat_responses, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    def run_gemini(self, prompt_type: str, dataset_name: str, model_name: str, examples: List[Dict]):\n",
    "        \"\"\"\n",
    "        Run benchmarks with Gemini models using their specific API format\n",
    "        \"\"\"\n",
    "        chat_responses = []\n",
    "        \n",
    "        for i in tqdm(range(min(self.sample_size, len(examples))), desc=f\"Gemini {model_name} on {dataset_name}\"):\n",
    "            question, original_answer = examples[i]\n",
    "            \n",
    "            if prompt_type == \"few_shot\":\n",
    "                prompt = self.prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "                instructions = self.system_prompts[\"few_shot\"]\n",
    "            else:  # zero_shot\n",
    "                prompt = question\n",
    "                instructions = self.system_prompts[\"zero_shot\"]\n",
    "            \n",
    "            try:\n",
    "                # Use the global genai module instead of self.gemini_client\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        system_instruction=instructions\n",
    "                    ),\n",
    "                    contents=prompt\n",
    "                )\n",
    "                model_answer = response.text\n",
    "            except Exception as e:\n",
    "                print(f\"Error with Gemini model {model_name}: {e}\")\n",
    "                model_answer = f\"ERROR: {str(e)}\"\n",
    "            \n",
    "            chat_responses.append({\n",
    "                \"question\": question,\n",
    "                \"original_answer\": original_answer,\n",
    "                \"answer_text\": model_answer\n",
    "            })\n",
    "            \n",
    "            # Add larger delay to avoid rate limiting\n",
    "            time.sleep(15)\n",
    "        \n",
    "        # Save results\n",
    "        output_filename = os.path.join(\n",
    "            self.output_dir,\n",
    "            f\"{dataset_name}-{prompt_type}-{model_name.replace('.', '-')}.json\"\n",
    "        )\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chat_responses, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "    def test_models(self) -> Dict[str, bool]:\n",
    "        \"\"\"Test if all models configured in the runner are working.\"\"\"\n",
    "        return test_models(self.openai_models, self.gemini_models)\n",
    "        \n",
    "    def prepare_examples(self, dataset_name: str):\n",
    "        \"\"\"\n",
    "        Process examples from the dataset using the appropriate template function\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of the dataset\n",
    "            \n",
    "        Returns:\n",
    "            List of (question, answer) tuples\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        # if dataset_name == \"csqa\":\n",
    "        #     dataset_examples = self.datasets[\"csqa\"][\"train\"]\n",
    "        if dataset_name == \"strategyqa\":\n",
    "            dataset_examples = self.datasets[\"strategyqa\"][\"train\"]\n",
    "        elif dataset_name == \"date\":\n",
    "            dataset_examples = self.datasets[\"date\"][\"examples\"]\n",
    "        elif dataset_name == \"sports\":\n",
    "            dataset_examples = self.datasets[\"sports\"][\"examples\"]\n",
    "        elif dataset_name == \"saycan\":\n",
    "            dataset_examples = self.datasets[\"saycan\"][\"test\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "        \n",
    "            \n",
    "        template_func = self.template_functions[dataset_name]\n",
    "        \n",
    "        # Process at most sample_size examples\n",
    "        for i in range(min(self.sample_size, len(dataset_examples))):\n",
    "            example = dataset_examples[i]\n",
    "            question, answer = template_func(example)\n",
    "            examples.append((question, answer))\n",
    "            \n",
    "        return examples\n",
    "    \n",
    "    def run_all_benchmarks(self, modes=[\"few_shot\", \"zero_shot\"], test_first=True):\n",
    "        \"\"\"\n",
    "        Run benchmarks for all datasets, models and specified modes\n",
    "        \n",
    "        Args:\n",
    "            modes: List of modes to run (\"few_shot\", \"zero_shot\")\n",
    "            test_first: Whether to test models before running benchmarks\n",
    "        \"\"\"\n",
    "        if test_first:\n",
    "            model_status = self.test_models()\n",
    "            working_models = [model for model, status in model_status.items() if status]\n",
    "            failed_models = [model for model, status in model_status.items() if not status]\n",
    "            \n",
    "            if failed_models:\n",
    "                print(f\"⚠️ WARNING: The following models failed the test: {', '.join(failed_models)}\")\n",
    "                proceed = input(\"Do you want to proceed with the working models only? (y/n): \")\n",
    "                if proceed.lower() != 'y':\n",
    "                    print(\"Benchmark run canceled.\")\n",
    "                    return\n",
    "                \n",
    "                # Filter models to only include working ones\n",
    "                self.openai_models = [model for model in self.openai_models if model in working_models]\n",
    "                self.gemini_models = [model for model in self.gemini_models if model in working_models]\n",
    "                \n",
    "                print(f\"Continuing with working models:\")\n",
    "                print(f\"  OpenAI models: {self.openai_models}\")\n",
    "                print(f\"  Gemini models: {self.gemini_models}\")\n",
    "        \n",
    "        for dataset_name in self.prompt_templates.keys():\n",
    "            # Prepare examples for this dataset\n",
    "            examples = self.prepare_examples(dataset_name)\n",
    "            \n",
    "            for mode in modes:\n",
    "                # Run OpenAI models\n",
    "                for model_name in self.openai_models:\n",
    "                    self.run_openai(mode, dataset_name, model_name, examples)\n",
    "                \n",
    "                # Run Gemini models\n",
    "                for model_name in self.gemini_models:\n",
    "                    self.run_gemini(mode, dataset_name, model_name, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e4df16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini gemini-1.5-flash on strategyqa:  10%|█         | 2/20 [01:27<13:44, 45.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Gemini model gemini-1.5-flash: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini gemini-1.5-flash on strategyqa:  15%|█▌        | 3/20 [01:46<09:27, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Gemini model gemini-1.5-flash: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini gemini-1.5-flash on strategyqa:  20%|██        | 4/20 [02:03<07:09, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Gemini model gemini-1.5-flash: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini gemini-1.5-flash on strategyqa:  25%|██▌       | 5/20 [02:24<06:13, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Gemini model gemini-1.5-flash: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini gemini-1.5-flash on strategyqa:  30%|███       | 6/20 [03:29<08:09, 34.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      4\u001b[39m runner = BenchmarkRunner(\n\u001b[32m      5\u001b[39m     datasets=datasets,\n\u001b[32m      6\u001b[39m     template_functions=template_functions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     sample_size=\u001b[32m20\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run benchmarks (test_first=False since we already tested)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_all_benchmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero_shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mBenchmarkRunner.run_all_benchmarks\u001b[39m\u001b[34m(self, modes, test_first)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# Run Gemini models\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gemini_models:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mBenchmarkRunner.run_gemini\u001b[39m\u001b[34m(self, prompt_type, dataset_name, model_name, examples)\u001b[39m\n\u001b[32m    167\u001b[39m     instructions = \u001b[38;5;28mself\u001b[39m.system_prompts[\u001b[33m\"\u001b[39m\u001b[33mzero_shot\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# Use the global genai module instead of self.gemini_client\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstructions\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     model_answer = response.text\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/google/genai/models.py:5898\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5896\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5897\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5898\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5899\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5900\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5901\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5902\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/google/genai/models.py:4838\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4835\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4836\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4838\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4842\u001b[39m response_dict = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   4844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/google/genai/_api_client.py:1077\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1069\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1072\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1073\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1074\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1075\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1076\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1078\u001b[39m   response_body = (\n\u001b[32m   1079\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1080\u001b[39m   )\n\u001b[32m   1081\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/google/genai/_api_client.py:968\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request\u001b[39m(\n\u001b[32m    964\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    965\u001b[39m     http_request: HttpRequest,\n\u001b[32m    966\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    967\u001b[39m ) -> HttpResponse:\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/google/genai/_api_client.py:951\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    947\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    948\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    949\u001b[39m   )\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m   errors.APIError.raise_for_response(response)\n\u001b[32m    959\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    960\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    961\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create benchmark runner with validated models\n",
    "    runner = BenchmarkRunner(\n",
    "        datasets=datasets,\n",
    "        template_functions=template_functions,\n",
    "        prompt_templates=prompt_templates,\n",
    "        openai_models=openai_models,\n",
    "        gemini_models=gemini_models,\n",
    "        system_prompts=system_prompts,\n",
    "        output_dir=\"results\",\n",
    "        sample_size=20\n",
    "    )\n",
    "    \n",
    "    # Run benchmarks (test_first=False since we already tested)\n",
    "    runner.run_all_benchmarks(modes=[\"zero_shot\"], test_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4be392b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 9741\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 1221\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 1140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41644ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full benchmark runner with OpenAI and Gemini client support + Gemini key rotation\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Callable\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Install dependencies if needed:\n",
    "# pip install openai google-generativeai tqdm python-dotenv\n",
    "\n",
    "# OpenAI Setup\n",
    "from openai import OpenAI\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Gemini Setup\n",
    "from google import genai\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, InvalidArgument\n",
    "\n",
    "# Load Gemini API keys\n",
    "def load_gemini_keys(file_path=\"gemini_keys.txt\"):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        keys = [line.strip() for line in f if line.strip()]\n",
    "    if not keys:\n",
    "        raise ValueError(\"No Gemini API keys found in file.\")\n",
    "    return keys\n",
    "\n",
    "# Rotating Gemini client\n",
    "class GeminiClientWithRotation:\n",
    "    def __init__(self, keys: List[str], model_name: str = \"gemini-pro\"):\n",
    "        self.keys = keys\n",
    "        self.model_name = model_name\n",
    "        self.current_key_index = 0\n",
    "        self.configure_genai(self.keys[self.current_key_index])\n",
    "\n",
    "    def configure_genai(self, key: str):\n",
    "        genai.configure(api_key=key)\n",
    "        self.model = genai.GenerativeModel(self.model_name)\n",
    "\n",
    "    def rotate_key(self):\n",
    "        self.current_key_index += 1\n",
    "        if self.current_key_index >= len(self.keys):\n",
    "            raise RuntimeError(\"All Gemini API keys failed.\")\n",
    "        self.configure_genai(self.keys[self.current_key_index])\n",
    "\n",
    "    def generate(self, prompt: str, instructions: str = \"\", temperature: float = 0.0):\n",
    "        retries = 0\n",
    "        while retries < len(self.keys):\n",
    "            try:\n",
    "                response = self.model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=genai.types.GenerationConfig(temperature=temperature),\n",
    "                    safety_settings=None,\n",
    "                    system_instruction=instructions\n",
    "                )\n",
    "                return response.text\n",
    "            except (GoogleAPICallError, RetryError, InvalidArgument) as e:\n",
    "                print(f\"❌ Gemini Key {self.keys[self.current_key_index][:8]}... failed: {type(e).__name__}\")\n",
    "                retries += 1\n",
    "                self.rotate_key()\n",
    "                time.sleep(1)\n",
    "        raise RuntimeError(\"Failed to generate response with all Gemini API keys.\")\n",
    "\n",
    "# Test OpenAI and Gemini models\n",
    "def test_models(openai_models, gemini_models, gemini_keys):\n",
    "    results = {}\n",
    "    test_prompt = \"Complete this sentence: The sky is\"\n",
    "\n",
    "    print(\"Testing OpenAI models...\")\n",
    "    for model in openai_models:\n",
    "        print(f\"  Testing {model}...\", end=\"\", flush=True)\n",
    "        try:\n",
    "            response = openai_client.responses.create(\n",
    "                model=model,\n",
    "                instructions=\"You are a helpful assistant.\",\n",
    "                input=test_prompt,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            print(f\" ✓ {response.output_text[:30]}...\")\n",
    "            results[model] = True\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Failed: {str(e)[:100]}\")\n",
    "            results[model] = False\n",
    "\n",
    "    print(\"\\nTesting Gemini models...\")\n",
    "    for model in gemini_models:\n",
    "        print(f\"  Testing {model}...\", end=\"\", flush=True)\n",
    "        try:\n",
    "            client = GeminiClientWithRotation(gemini_keys, model_name=model)\n",
    "            response = client.generate(test_prompt)\n",
    "            print(f\" ✓ {response[:30]}...\")\n",
    "            results[model] = True\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Failed: {str(e)[:100]}\")\n",
    "            results[model] = False\n",
    "\n",
    "    return results\n",
    "\n",
    "# Benchmark Runner\n",
    "class BenchmarkRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        datasets: Dict[str, any],\n",
    "        template_functions: Dict[str, Callable],\n",
    "        prompt_templates: Dict[str, str],\n",
    "        openai_models: List[str],\n",
    "        gemini_models: List[str],\n",
    "        system_prompts: Dict[str, str],\n",
    "        gemini_keys: List[str],\n",
    "        output_dir: str = \"results\",\n",
    "        sample_size: int = 20\n",
    "    ):\n",
    "        self.datasets = datasets\n",
    "        self.template_functions = template_functions\n",
    "        self.prompt_templates = prompt_templates\n",
    "        self.openai_models = openai_models\n",
    "        self.gemini_models = gemini_models\n",
    "        self.system_prompts = system_prompts\n",
    "        self.gemini_keys = gemini_keys\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def prepare_examples(self, dataset_name: str):\n",
    "        examples = []\n",
    "        if dataset_name in self.datasets:\n",
    "            dataset_examples = self.datasets[dataset_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "        template_func = self.template_functions[dataset_name]\n",
    "        for i in range(min(self.sample_size, len(dataset_examples))):\n",
    "            example = dataset_examples[i]\n",
    "            question, answer = template_func(example)\n",
    "            examples.append((question, answer))\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def run_openai(self, prompt_type, dataset_name, model_name, examples):\n",
    "        results = []\n",
    "\n",
    "        for i in tqdm(range(len(examples)), desc=f\"OpenAI {model_name} on {dataset_name}\"):\n",
    "            question, original_answer = examples[i]\n",
    "\n",
    "            if prompt_type == \"few_shot\":\n",
    "                prompt = self.prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "                instructions = self.system_prompts[\"few_shot\"]\n",
    "            else:\n",
    "                prompt = question\n",
    "                instructions = self.system_prompts[\"zero_shot\"]\n",
    "\n",
    "            try:\n",
    "                response = openai_client.responses.create(\n",
    "                    model=model_name,\n",
    "                    instructions=instructions,\n",
    "                    input=prompt,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "                answer = response.output_text\n",
    "            except Exception as e:\n",
    "                answer = f\"ERROR: {str(e)}\"\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"original_answer\": original_answer,\n",
    "                \"answer_text\": answer\n",
    "            })\n",
    "\n",
    "        with open(os.path.join(self.output_dir, f\"{dataset_name}-{prompt_type}-{model_name}.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def run_gemini(self, prompt_type, dataset_name, model_name, examples):\n",
    "        results = []\n",
    "        client = GeminiClientWithRotation(self.gemini_keys, model_name=model_name)\n",
    "\n",
    "        for i in tqdm(range(len(examples)), desc=f\"Gemini {model_name} on {dataset_name}\"):\n",
    "            question, original_answer = examples[i]\n",
    "\n",
    "            if prompt_type == \"few_shot\":\n",
    "                prompt = self.prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "                instructions = self.system_prompts[\"few_shot\"]\n",
    "            else:\n",
    "                prompt = question\n",
    "                instructions = self.system_prompts[\"zero_shot\"]\n",
    "\n",
    "            try:\n",
    "                answer = client.generate(prompt, instructions=instructions)\n",
    "            except Exception as e:\n",
    "                answer = f\"ERROR: {str(e)}\"\n",
    "\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"original_answer\": original_answer,\n",
    "                \"answer_text\": answer\n",
    "            })\n",
    "            time.sleep(2)\n",
    "\n",
    "        with open(os.path.join(self.output_dir, f\"{dataset_name}-{prompt_type}-{model_name}.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def run_all_benchmarks(self, modes=[\"few_shot\", \"zero_shot\"], test_first=True):\n",
    "        if test_first:\n",
    "            model_status = test_models(self.openai_models, self.gemini_models, self.gemini_keys)\n",
    "            self.openai_models = [m for m in self.openai_models if model_status.get(m, False)]\n",
    "            self.gemini_models = [m for m in self.gemini_models if model_status.get(m, False)]\n",
    "\n",
    "        for dataset_name in self.prompt_templates:\n",
    "            examples = self.prepare_examples(dataset_name)\n",
    "            for mode in modes:\n",
    "                for model in self.openai_models:\n",
    "                    self.run_openai(mode, dataset_name, model, examples)\n",
    "                for model in self.gemini_models:\n",
    "                    self.run_gemini(mode, dataset_name, model, examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe6547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
