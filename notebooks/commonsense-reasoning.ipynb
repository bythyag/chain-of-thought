{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc318f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import itertools\n",
    "from typing import Dict, List, Callable, Any\n",
    "from pprint import pprint\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model and AI libraries\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai\n",
    "load_dotenv()\n",
    "# Configure OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# 1. CSQA – CommonsenseQA (train/dev/test splits)\n",
    "csqa = load_dataset(\"commonsense_qa\")          \n",
    "\n",
    "# 2. StrategyQA – yes/no questions requiring multi-hop reasoning\n",
    "strategyqa = load_dataset(\"metaeval/strategy-qa\")\n",
    "\n",
    "# 3. Date Understanding\n",
    "date_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\n",
    "date_response = requests.get(date_url)\n",
    "date = json.loads(date_response.text)\n",
    "\n",
    "# 4. Sports Understanding – sports-related questions\n",
    "sports_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/sports_understanding/task.json\"\n",
    "sports_response = requests.get(sports_url)\n",
    "sports = json.loads(sports_response.text)\n",
    "\n",
    "# 5. SayCan – Language→robot-action mapping (Google Robotics, 2022)\n",
    "saycan = load_dataset(\"chiayewken/saycan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template functions for commonsense reasoning tasks\n",
    "\n",
    "def csqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a CommonsenseQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question', 'choices', and 'answerKey'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    labels = [label.lower() for label in example['choices']['label']]\n",
    "    texts = example['choices']['text']\n",
    "    choices_str = \" \".join([f\"({label}) {text}\" for label, text in zip(labels, texts)])\n",
    "    question_template = f\"{example['question']} Answer Choices: {choices_str}\"\n",
    "    answer = f\"The answer is ({example['answerKey'].lower()})\"\n",
    "    return question_template, answer\n",
    "\n",
    "def strategyqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a StrategyQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'answer'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "    answer = \"yes\" if example['answer'] else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def date_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a date understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['input']\n",
    "    correct_answer = str(next((k for k, v in example['target_scores'].items() if v == 1), None))\n",
    "    answer = f\"The answer is {correct_answer}\"\n",
    "    return question, answer\n",
    "\n",
    "def sports_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a sports understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'input' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = f'Is the following sentence plausible? \"{example[\"input\"]}\"'\n",
    "    answer = \"yes\" if example[\"target_scores\"].get(\"plausible\", 0) == 1 else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def saycan_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a SayCan example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'instruction' and 'plan'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['INPUT']\n",
    "    answer = f\"The answer is {example['OUTPUT']}\"\n",
    "    return question, answer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b586f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick smoke test for templates\n",
    "\n",
    "REAL_EXAMPLES = {\n",
    "    \"csqa\": {\n",
    "        \"question\": \"Where would you find a hamburger?\",\n",
    "        \"choices\": {\n",
    "            \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "            \"text\": [\"fast food restaurant\", \"pizza\", \"kitchen\", \"grocery store\", \"farms\"]\n",
    "        },\n",
    "        \"answerKey\": \"A\"\n",
    "    },\n",
    "    \"strategyqa\": {\n",
    "        \"question\": \"Would a person with a high fever typically sweat?\",\n",
    "        \"answer\": True\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"input\": \"What day of the week was 2020-02-02?\",\n",
    "        \"target_scores\": {\"Sunday\": 1, \"Monday\": 0, \"Tuesday\": 0}\n",
    "    },\n",
    "    \"sports\": {\n",
    "        \"input\": \"The quarterback threw a touchdown pass.\",\n",
    "        \"target_scores\": {\"plausible\": 1}\n",
    "    },\n",
    "    \"saycan\": {\n",
    "        \"INPUT\": \"Bring me a cold drink.\",\n",
    "        \"PlAN\": \"open fridge, pick drink, close fridge, bring drink\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def test_smoke():\n",
    "    \"\"\"Sanity check with the hand-crafted examples above.\"\"\"\n",
    "    templates = {\n",
    "        \"csqa\": csqa_template,\n",
    "        \"strategyqa\": strategyqa_template,\n",
    "        \"date\": date_template,\n",
    "        \"sports\": sports_template,\n",
    "        \"saycan\": saycan_template,\n",
    "    }\n",
    "\n",
    "    for dataset, tpl in templates.items():\n",
    "        print(f\"\\n--- {dataset} ---\")\n",
    "        q, a = tpl(REAL_EXAMPLES[dataset])\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer  :\", a)\n",
    "        # basic assertions\n",
    "        assert isinstance(q, str) and q.strip()\n",
    "        assert isinstance(a, str) and a.strip()\n",
    "\n",
    "test_smoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for CSQA\n",
    "\n",
    "CSQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\n",
    "calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.\n",
    "So the answer is (e).\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
    "A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\n",
    "Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b)\n",
    "hen house (c) natural habitat (d) storybook\n",
    "A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the\n",
    "answer is (b).\n",
    "Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\n",
    "(b) race track (c) desert (d) apartment (e) roadblock\n",
    "A: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of\n",
    "people. So the answer is (a).\n",
    "Q: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\n",
    "market (d) fruit basket (e) fruit market\n",
    "A: The answer should be the place where grocery items are placed before checking out. Of the above choices,\n",
    "grocery cart makes the most sense for holding grocery items. So the answer is (b).\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A: The answer must be something that used to do what Google Maps and GPS services do, which is to give\n",
    "directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\n",
    "Q: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\n",
    "anguish (c) bitterness (d) tears (e) sadness\n",
    "A: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above\n",
    "choices, the closest feeling is bitterness. So the answer is (c).\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b798037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for StrategyQA\n",
    "\n",
    "StrategyQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: Do hamsters provide food for any animals?\n",
    "A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So\n",
    "the answer is yes.\n",
    "Q: Could Brooke Shields succeed at University of Pennsylvania?\n",
    "A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\n",
    "University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the\n",
    "answer is yes.\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic\n",
    "number squared is less than 5. So the answer is no.\n",
    "Q: Yes or no: Is it common to see frost during some college commencements?\n",
    "A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\n",
    "there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\n",
    "Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n",
    "A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\n",
    "months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\n",
    "Q: Yes or no: Would a pear sink in water?\n",
    "A: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus,\n",
    "a pear would float. So the answer is no.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for date understanding \n",
    "\n",
    "Date_PROMPT = \"\"\"\n",
    "\n",
    "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today\n",
    "is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.\n",
    "Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in\n",
    "MM/DD/YYYY?\n",
    "A: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first monday, would be six\n",
    "days later. So today is 01/07/2019. So the answer is 01/07/2019.\n",
    "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\n",
    "days ago in MM/DD/YYYY?\n",
    "A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the\n",
    "answer is 05/23/1943.\n",
    "Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is\n",
    "04/20/1969.\n",
    "Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\n",
    "later in MM/DD/YYYY?\n",
    "A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\n",
    "Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\n",
    "yesterday in MM/DD/YYYY?\n",
    "A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so\n",
    "today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ebddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for sports understanding\n",
    "\n",
    "Sports_PROMPT = \"\"\"\n",
    "\n",
    "Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\n",
    "A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\n",
    "A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”\n",
    "A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer\n",
    "is no.\n",
    "Q: Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\n",
    "A: Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Jamel Murray was perfect from the line.”\n",
    "A: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Sam Darnold passed the puck.”\n",
    "A: Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A: Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\n",
    "A: Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. So the answer is yes.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c971580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for saycan \n",
    "\n",
    "SayCan_PROMPT = \"\"\"\n",
    "\n",
    "Locations = [counter, table, user, trash, bowl].\n",
    "Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\n",
    "grapefruit soda, pepsi, redbull, energy bar, sponge, water].\n",
    "The robot can pick up items with pick(object) and put down items with put(object) as well as find objects or\n",
    "locations with find(). The robot can only understand the explicit locations and objects listed.\n",
    "Human: I’m hungry, can you bring me some chips.\n",
    "Explanation: The user is hungry and has asked for chips. There are several types of chips available, I will bring\n",
    "the user the kettle chips.\n",
    "Plan: 1. find(kettle chips), 2. pick(kettle chips), 3. find(user), 4. put(kettle chips), 5. done().\n",
    "Human: How would you move the grapefruit drink from the table to the counter?\n",
    "Explanation: The user has asked me to move the grapefruit drink to the counter.\n",
    "Plan: 1. find(grapefruit soda), 2. pick(grapefruit soda), 3. find(counter), 4. put(grapefruit soda), 5. done().\n",
    "Human: How would you bring me some snacks?\n",
    "Explanation: The user has asked for snacks, I will choose two items and bring them. I will bring jalapeno chips\n",
    "and an apple.\n",
    "Plan: 1. find(jalapeno chips), 2. pick(jalapeno chips), 3. find(user), 4. put(jalapeno chips), 5. find(apple), 6.\n",
    "pick(apple), 7. find(user), 8. put(apple), 9. done().\n",
    "Human: How would you bring me something to eat that isn’t a fruit?\n",
    "Explanation: The user has asked for a food that isn’t a fruit, I will bring an energy bar to them.\n",
    "Plan: 1. find(energy bar), 2. pick(energy bar), 3. find(user), 4. put(energy bar), 5. done().\n",
    "Human: How would you put the rice chips in the bowl and then move the tea to the table?\n",
    "Explanation: The user has asked me to do two tasks, I will do one and then the other.\n",
    "Plan: 1. find(rice chips), 2. pick(rice chips), 3. find(bowl), 4. put(rice chips), 5. find(tea), 6. pick(tea), 7.\n",
    "find(table), 8. put(tea), 9. done().\n",
    "Human: How would you throw away a redbull?\n",
    "Explanation: The user has asked me to throw away the redbull, I will move it to the trash.\n",
    "Plan: 1. find(redbull), 2. pick(redbull), 3. find(trash), 4. put(redbull), 5. done().\n",
    "Human: Bring me a drink.\n",
    "Explanation: The user has asked for a drink and there are many options. I will bring them a water.\n",
    "Plan: 1. find(water), 2. pick(water), 3. find(user), 4. put(water), 5. done().\n",
    "\n",
    "\n",
    "Human: {Question}\n",
    "Explanation:\n",
    "Plan:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up template functions\n",
    "template_functions = {\n",
    "    \"csqa\": csqa_template,\n",
    "    \"strategyqa\": strategyqa_template,\n",
    "    \"date\": date_template,\n",
    "    \"sports\": sports_template,\n",
    "    \"saycan\": saycan_template\n",
    "}\n",
    "\n",
    "# Set up prompt templates\n",
    "prompt_templates = {\n",
    "    \"csqa\": CSQA_PROMPT,\n",
    "    \"strategyqa\": StrategyQA_PROMPT,\n",
    "    \"date\": Date_PROMPT,\n",
    "    \"sports\": Sports_PROMPT,\n",
    "    \"saycan\": SayCan_PROMPT\n",
    "}\n",
    "\n",
    "# Set up datasets\n",
    "datasets = {\n",
    "    \"csqa\": csqa,\n",
    "    \"strategyqa\": strategyqa,\n",
    "    \"date\": date,\n",
    "    \"sports\": sports,\n",
    "    \"saycan\": saycan\n",
    "}\n",
    "\n",
    "# Define models and system prompts\n",
    "openai_models = [\"gpt-4.1-nano\", \"gpt-4o-mini-2024-07-18\"]\n",
    "gemini_models = [ \"gemini-1.5-flash\", \"gemini-2.0-flash\"]\n",
    "system_prompts = {\n",
    "    \"few_shot\": \"You are a helpful assistant. Carefully follow the reasoning and answer format shown in the few-shot examples provided by the user. Use the same step-by-step explanation style and answer format.\",\n",
    "    \"zero_shot\": \"You are a helpful assistant. First, provide a detailed answer to the question. Then, on a new line, clearly state your final answer in the format: The answer is {answer}. Only include the final answer in that line, without any explanation.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a00e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code to extract data from Gemini, OpenAI and Gemma 3 1B IT\n",
    "\n",
    "gemini_keys = [\n",
    "# add gemini keys here\n",
    "]\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    def __init__(self,\n",
    "                 datasets: Dict[str, Any],\n",
    "                 template_functions: Dict[str, Callable],\n",
    "                 prompt_templates: Dict[str, str],\n",
    "                 openai_models: List[str],\n",
    "                 gemini_models: List[str],\n",
    "                 system_prompts: Dict[str, str],\n",
    "                 gemini_keys: List[str],\n",
    "                 output_dir: str = \"results\",\n",
    "                 sample_size: int = 20):\n",
    "        self.datasets = datasets\n",
    "        self.template_functions = template_functions\n",
    "        self.prompt_templates = prompt_templates\n",
    "        self.openai_models = openai_models\n",
    "        self.gemini_models = gemini_models\n",
    "        self.system_prompts = system_prompts\n",
    "        self.gemini_keys = gemini_keys\n",
    "        self.key_cycle = itertools.cycle(gemini_keys)\n",
    "        self.openai_client = OpenAI()\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = sample_size\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def _get_next_key(self) -> str:\n",
    "        return next(self.key_cycle)\n",
    "\n",
    "    def _save_results(self, results: List[Dict], dataset_name: str, prompt_type: str, model_name: str):\n",
    "        path = os.path.join(self.output_dir, f\"{dataset_name}-{prompt_type}-{model_name}.json\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(\"Saved:\", path)\n",
    "\n",
    "    def run_openai(self, prompt_type: str, dataset_name: str, model_name: str, examples: List[Tuple[str, Any]]):\n",
    "        results = []\n",
    "        instructions = self.system_prompts[prompt_type]\n",
    "        for question, original in tqdm(examples, desc=f\"OpenAI {model_name}\"):\n",
    "            prompt = self._prepare_prompt(prompt_type, dataset_name, question)\n",
    "            try:\n",
    "                resp = self.openai_client.responses.create(\n",
    "                    model=model_name,\n",
    "                    instructions=instructions,\n",
    "                    input=prompt,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "                answer = resp.output_text\n",
    "            except Exception as e:\n",
    "                answer = f\"ERROR: {e}\"\n",
    "            results.append({\"question\": question, \"original\": original, \"answer\": answer})\n",
    "        self._save_results(results, dataset_name, prompt_type, model_name)\n",
    "\n",
    "    def run_gemini(self, prompt_type: str, dataset_name: str, model_name: str, examples: List[Tuple[str, Any]]):\n",
    "        results = []\n",
    "        instructions = self.system_prompts[prompt_type]\n",
    "        for question, original in tqdm(examples, desc=f\"Gemini {model_name}\"):\n",
    "            prompt = self._prepare_prompt(prompt_type, dataset_name, question)\n",
    "            max_retries = len(self.gemini_keys)\n",
    "            for _ in range(max_retries):\n",
    "                key = self._get_next_key()\n",
    "                client = genai.Client(api_key=key)\n",
    "                try:\n",
    "                    resp = client.models.generate_content(\n",
    "                        model=model_name,\n",
    "                        contents=prompt,\n",
    "                        config=types.GenerateContentConfig(system_instruction=instructions)\n",
    "                    )\n",
    "                    answer = resp.text\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    answer = f\"ERROR: {e}\"\n",
    "                    time.sleep(1)\n",
    "            results.append({\"question\": question, \"original\": original, \"answer\": answer})\n",
    "        self._save_results(results, dataset_name, prompt_type, model_name)\n",
    "\n",
    "    def _prepare_prompt(self, prompt_type: str, dataset_name: str, question: str) -> str:\n",
    "        return self.prompt_templates[dataset_name].replace(\"{Question}\", question) if prompt_type == \"few_shot\" else question\n",
    "\n",
    "\n",
    "    def prepare_examples(self, dataset_name: str):\n",
    "            examples = []\n",
    "            \n",
    "            if dataset_name == \"csqa\":\n",
    "                 dataset_examples = self.datasets[\"csqa\"][\"train\"]\n",
    "            elif dataset_name == \"strategyqa\":\n",
    "                dataset_examples = self.datasets[\"strategyqa\"][\"train\"]\n",
    "            elif dataset_name == \"date\":\n",
    "                dataset_examples = self.datasets[\"date\"][\"examples\"]\n",
    "            elif dataset_name == \"sports\":\n",
    "                dataset_examples = self.datasets[\"sports\"][\"examples\"]\n",
    "            elif dataset_name == \"saycan\":\n",
    "                dataset_examples = self.datasets[\"saycan\"][\"test\"]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "            \n",
    "                \n",
    "            template_func = self.template_functions[dataset_name]\n",
    "            \n",
    "            # Process at most sample_size examples\n",
    "            for i in range(min(self.sample_size, len(dataset_examples))):\n",
    "                example = dataset_examples[i]\n",
    "                question, answer = template_func(example)\n",
    "                examples.append((question, answer))\n",
    "                \n",
    "            return examples\n",
    "\n",
    "    def run_all_benchmarks(self, modes: List[str] = [\"few_shot\", \"zero_shot\"], test_first: bool = True):\n",
    "        if test_first:\n",
    "            working_models = self._test_models()\n",
    "            self.openai_models = [m for m in self.openai_models if m in working_models]\n",
    "            self.gemini_models = [m for m in self.gemini_models if m in working_models]\n",
    "\n",
    "        for dataset_name in self.prompt_templates.keys():\n",
    "            examples = self.prepare_examples(dataset_name)\n",
    "            for mode in modes:\n",
    "                for model_name in self.openai_models:\n",
    "                    self.run_openai(mode, dataset_name, model_name, examples)\n",
    "                for model_name in self.gemini_models:\n",
    "                    self.run_gemini(mode, dataset_name, model_name, examples)\n",
    "\n",
    "    def _test_models(self) -> List[str]:\n",
    "        test_prompt = \"Complete this sentence: The sky is\"\n",
    "        working_models = []\n",
    "        print(\"Testing OpenAI models...\")\n",
    "        for model in self.openai_models:\n",
    "            try:\n",
    "                self.openai_client.responses.create(\n",
    "                    model=model,\n",
    "                    instructions=\"You are a helpful assistant.\",\n",
    "                    input=test_prompt,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "                working_models.append(model)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        print(\"Testing Gemini models...\")\n",
    "        for model in self.gemini_models:\n",
    "            for _ in range(len(self.gemini_keys)):\n",
    "                key = self._get_next_key()\n",
    "                client = genai.Client(api_key=key)\n",
    "                try:\n",
    "                    client.models.generate_content(\n",
    "                        model=model,\n",
    "                        contents=test_prompt,\n",
    "                        config=types.GenerateContentConfig(temperature=0.0)\n",
    "                    )\n",
    "                    working_models.append(model)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        return working_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4df16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "runner = BenchmarkRunner(\n",
    "    datasets=datasets,\n",
    "    template_functions=template_functions,\n",
    "    prompt_templates=prompt_templates,\n",
    "    openai_models=openai_models,\n",
    "    gemini_models=gemini_models,\n",
    "    system_prompts=system_prompts,\n",
    "    gemini_keys=gemini_keys,\n",
    "    output_dir=\"results\",\n",
    "    sample_size=20\n",
    ")\n",
    "\n",
    "# Run benchmarks (test_first=False since we already tested)\n",
    "runner.run_all_benchmarks(modes=[\"zero_shot\", \"few_shot\"], test_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096364d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in case gemini 1.5 Flash API keys need to be rotated due to errors in the JSON files\n",
    "\n",
    "def fix_gemini_errors(results_dir: str = \"results\"):\n",
    "    \"\"\"\n",
    "    Find and fix errors in Gemini 1.5 Flash JSON files by retrying with key rotation.\n",
    "    \"\"\"\n",
    "    # Gemini API keys for rotation\n",
    "    gemini_keys = [\n",
    "        # Add your Gemini API keys here\n",
    "    ]\n",
    "    key_index = 0\n",
    "    \n",
    "    # Define system prompts\n",
    "    system_prompts = {\n",
    "        \"few_shot\": \"You are a helpful assistant. Carefully follow the reasoning and answer format shown in the few-shot examples provided by the user. Use the same step-by-step explanation style and answer format.\",\n",
    "        \"zero_shot\": \"You are a helpful assistant. First, provide a detailed answer to the question. Then, on a new line, clearly state your final answer in the format: The answer is {answer}. Only include the final answer in that line, without any explanation.\"\n",
    "    }\n",
    "    \n",
    "    # Load prompt templates from notebook\n",
    "    prompt_templates = {\n",
    "        \"csqa\": CSQA_PROMPT,\n",
    "        \"strategyqa\": StrategyQA_PROMPT,\n",
    "        \"date\": Date_PROMPT,\n",
    "        \"sports\": Sports_PROMPT,\n",
    "        \"saycan\": SayCan_PROMPT\n",
    "    }\n",
    "    \n",
    "    # Function to get the next API key in rotation\n",
    "    def get_next_key():\n",
    "        nonlocal key_index\n",
    "        key = gemini_keys[key_index]\n",
    "        key_index = (key_index + 1) % len(gemini_keys)\n",
    "        return key\n",
    "    \n",
    "    # Function to retry a query with key rotation\n",
    "    def retry_query(question, dataset_name=None, prompt_type=None):\n",
    "        for attempt in range(len(gemini_keys)):\n",
    "            key = get_next_key()\n",
    "            client = genai.Client(api_key=key)\n",
    "            \n",
    "            try:\n",
    "                # Determine if this is few-shot or zero-shot\n",
    "                if prompt_type == \"few_shot\" and dataset_name in prompt_templates and prompt_templates[dataset_name]:\n",
    "                    prompt = prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "                    instructions = system_prompts[\"few_shot\"]\n",
    "                else:\n",
    "                    prompt = question\n",
    "                    instructions = system_prompts[\"zero_shot\"]\n",
    "                \n",
    "                resp = client.models.generate_content(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    contents=prompt,\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        system_instruction=instructions\n",
    "                    )\n",
    "                )\n",
    "                return resp.text, None\n",
    "            except Exception as e:\n",
    "                print(f\"Key {attempt+1} failed: {str(e)[:100]}...\")\n",
    "                last_error = e\n",
    "                time.sleep(2)  # Wait before trying the next key\n",
    "                \n",
    "        return None, f\"ERROR: All keys failed. Last error: {last_error}\"\n",
    "\n",
    "    # Find all Gemini files\n",
    "    gemini_files = glob.glob(os.path.join(results_dir, \"*gemini-1.5-flash*.json\"))\n",
    "    print(f\"Found {len(gemini_files)} Gemini 1.5 Flash result files.\")\n",
    "    \n",
    "    for file_path in gemini_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"\\nProcessing: {file_name}\")\n",
    "        \n",
    "        # Extract dataset name and prompt type from filename\n",
    "        parts = file_name.replace(\".json\", \"\").split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"  Skipping: Cannot parse filename format for {file_name}\")\n",
    "            continue\n",
    "            \n",
    "        dataset_name = parts[0]\n",
    "        prompt_type = parts[1]  # \"few_shot\" or \"zero_shot\"\n",
    "        \n",
    "        # Load the JSON data\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Count errors\n",
    "        error_count = sum(1 for item in data if item.get(\"answer\", \"\").startswith(\"ERROR:\"))\n",
    "        if error_count == 0:\n",
    "            print(f\"  No errors found in {file_name}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Found {error_count} errors to fix in {file_name}\")\n",
    "        \n",
    "        # Process each error\n",
    "        fixed_count = 0\n",
    "        for i, item in enumerate(tqdm(data, desc=f\"Fixing errors in {file_name}\")):\n",
    "            if item.get(\"answer\", \"\").startswith(\"ERROR:\"):\n",
    "                question = item[\"question\"]\n",
    "                print(f\"\\n  Retrying question: {question[:50]}...\")\n",
    "                \n",
    "                # Retry the query\n",
    "                new_answer, error = retry_query(question, dataset_name, prompt_type)\n",
    "                \n",
    "                if error:\n",
    "                    print(f\"  Failed to fix item {i+1} after trying all keys\")\n",
    "                    continue\n",
    "                    \n",
    "                # Update the item with the new answer\n",
    "                data[i][\"answer\"] = new_answer\n",
    "                fixed_count += 1\n",
    "                print(f\"  Fixed item {i+1}\")\n",
    "                time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Save the updated file\n",
    "        if fixed_count > 0:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"  Saved {fixed_count}/{error_count} fixed items in {file_name}\")\n",
    "        else:\n",
    "            print(f\"  No items were fixed in {file_name}\")\n",
    "\n",
    "# Run the script\n",
    "fix_gemini_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a620bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ollama 3.1 3B IT model to run the benchmarks\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    def __init__(self,\n",
    "                 datasets: Dict[str, Any],\n",
    "                 template_functions: Dict[str, Callable],\n",
    "                 prompt_templates: Dict[str, str],\n",
    "                 system_prompts: Dict[str, str],\n",
    "                 output_dir: str = \"results\",\n",
    "                 sample_size: int = 20,\n",
    "                 ollama_url: str = \"http://localhost:11434/api/generate\",\n",
    "                 gemma_model: str = \"gemma:3b\"):\n",
    "        self.datasets = datasets\n",
    "        self.template_functions = template_functions\n",
    "        self.prompt_templates = prompt_templates\n",
    "        self.system_prompts = system_prompts\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.ollama_url = ollama_url\n",
    "        self.gemma_model = gemma_model\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def _save_results(self, results: List[Dict], dataset_name: str, prompt_type: str):\n",
    "        path = os.path.join(self.output_dir, f\"{dataset_name}-{prompt_type}-{self.gemma_model.replace(':', '_')}.json\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(\"Saved:\", path)\n",
    "\n",
    "    def _prepare_prompt(self, prompt_type: str, dataset_name: str, question: str, system_prompt: str) -> str:\n",
    "        prompt = question if prompt_type == \"zero_shot\" else self.prompt_templates[dataset_name].replace(\"{Question}\", question)\n",
    "        if system_prompt:\n",
    "            return f\"{system_prompt.strip()}\\n\\n{prompt.strip()}\"\n",
    "        return prompt.strip()\n",
    "\n",
    "    def run_gemma(self, prompt_type: str, dataset_name: str, examples: List[Tuple[str, Any]]):\n",
    "        results = []\n",
    "        system_prompt = self.system_prompts.get(prompt_type, \"\")\n",
    "\n",
    "        for question, original in tqdm(examples, desc=f\"Gemma {self.gemma_model}\"):\n",
    "            prompt = self._prepare_prompt(prompt_type, dataset_name, question, system_prompt)\n",
    "            try:\n",
    "                response = requests.post(self.ollama_url, json={\n",
    "                    \"model\": self.gemma_model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                })\n",
    "                response.raise_for_status()\n",
    "                answer = response.json().get(\"response\", \"\")\n",
    "            except Exception as e:\n",
    "                answer = f\"ERROR: {e}\"\n",
    "            results.append({\"question\": question, \"original\": original, \"answer\": answer})\n",
    "\n",
    "        self._save_results(results, dataset_name, prompt_type)\n",
    "\n",
    "\n",
    "    def prepare_examples(self, dataset_name: str):\n",
    "            examples = []\n",
    "            \n",
    "            if dataset_name == \"csqa\":\n",
    "                 dataset_examples = self.datasets[\"csqa\"][\"train\"]\n",
    "            elif dataset_name == \"strategyqa\":\n",
    "                dataset_examples = self.datasets[\"strategyqa\"][\"train\"]\n",
    "            elif dataset_name == \"date\":\n",
    "                dataset_examples = self.datasets[\"date\"][\"examples\"]\n",
    "            elif dataset_name == \"sports\":\n",
    "                dataset_examples = self.datasets[\"sports\"][\"examples\"]\n",
    "            elif dataset_name == \"saycan\":\n",
    "                dataset_examples = self.datasets[\"saycan\"][\"test\"]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "            \n",
    "                \n",
    "            template_func = self.template_functions[dataset_name]\n",
    "            \n",
    "            # Process at most sample_size examples\n",
    "            for i in range(min(self.sample_size, len(dataset_examples))):\n",
    "                example = dataset_examples[i]\n",
    "                question, answer = template_func(example)\n",
    "                examples.append((question, answer))\n",
    "                \n",
    "            return examples\n",
    "\n",
    "    def run_all_benchmarks(self, modes: List[str] = [\"few_shot\", \"zero_shot\"]):\n",
    "        for dataset_name in self.prompt_templates.keys():\n",
    "            examples = self.prepare_examples(dataset_name)\n",
    "            for mode in modes:\n",
    "                self.run_gemma(mode, dataset_name, examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    runner = BenchmarkRunner(\n",
    "        datasets=datasets,\n",
    "        template_functions=template_functions,\n",
    "        prompt_templates=prompt_templates,\n",
    "        system_prompts=system_prompts,\n",
    "        output_dir=\"results\",\n",
    "        sample_size=20,\n",
    "        gemma_model=\"gemma3:1b\",  # or another variant like gemma:2b-it\n",
    "    )\n",
    "    runner.run_all_benchmarks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
