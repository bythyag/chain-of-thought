{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc318f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thyag/Desktop/COT/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from openai import OpenAI\n",
    "from google.genai import types\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import requests\n",
    "import gzip\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv('GEMINI_API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cd8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# 1. CSQA – CommonsenseQA (train/dev/test splits)\n",
    "csqa = load_dataset(\"commonsense_qa\")          \n",
    "\n",
    "# 2. StrategyQA – yes/no questions requiring multi-hop reasoning\n",
    "strategyqa = load_dataset(\"metaeval/strategy-qa\")\n",
    "\n",
    "# 3. Date Understanding\n",
    "date_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\n",
    "date_response = requests.get(date_url)\n",
    "date = json.loads(date_response.text)\n",
    "\n",
    "# 4. Sports Understanding – sports-related questions\n",
    "sports_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/sports_understanding/task.json\"\n",
    "sports_response = requests.get(sports_url)\n",
    "sports = json.loads(sports_response.text)\n",
    "\n",
    "# 5. SayCan – Language→robot-action mapping (Google Robotics, 2022)\n",
    "saycan = load_dataset(\"chiayewken/saycan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc4d5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template functions for commonsense reasoning tasks\n",
    "\n",
    "def csqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a CommonsenseQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question', 'choices', and 'answerKey'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    labels = [label.lower() for label in example['choices']['label']]\n",
    "    texts = example['choices']['text']\n",
    "    choices_str = \" \".join([f\"({label}) {text}\" for label, text in zip(labels, texts)])\n",
    "    question_template = f\"{example['question']} Answer Choices: {choices_str}\"\n",
    "    answer = f\"The answer is ({example['answerKey'].lower()})\"\n",
    "    return question_template, answer\n",
    "\n",
    "def strategyqa_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a StrategyQA example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'answer'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "    answer = \"yes\" if example['answer'] else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def date_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a date understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'question' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['input']\n",
    "    correct_answer = str(next((k for k, v in example['target_scores'].items() if v == 1), None))\n",
    "    answer = f\"The answer is {correct_answer}\"\n",
    "    return question, answer\n",
    "\n",
    "def sports_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a sports understanding example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'input' and 'target_scores'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = f'Is the following sentence plausible? \"{example[\"input\"]}\"'\n",
    "    answer = \"yes\" if example[\"target_scores\"].get(\"plausible\", 0) == 1 else \"no\"\n",
    "    answer_text = f\"The answer is {answer}\"\n",
    "    return question, answer_text\n",
    "\n",
    "def saycan_template(example: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a SayCan example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'instruction' and 'plan'.\n",
    "    Returns:\n",
    "        tuple: (question string, answer string)\n",
    "    \"\"\"\n",
    "    question = example['instruction']\n",
    "    answer = f\"The answer is {example['plan']}\"\n",
    "    return question, answer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b586f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- csqa ---\n",
      "Question: Where would you find a hamburger? Answer Choices: (a) fast food restaurant (b) pizza (c) kitchen (d) grocery store (e) farms\n",
      "Answer  : The answer is (a)\n",
      "\n",
      "--- strategyqa ---\n",
      "Question: Would a person with a high fever typically sweat?\n",
      "Answer  : The answer is yes\n",
      "\n",
      "--- date ---\n",
      "Question: What day of the week was 2020-02-02?\n",
      "Answer  : The answer is Sunday\n",
      "\n",
      "--- sports ---\n",
      "Question: Is the following sentence plausible? \"The quarterback threw a touchdown pass.\"\n",
      "Answer  : The answer is yes\n",
      "\n",
      "--- saycan ---\n",
      "Question: Bring me a cold drink.\n",
      "Answer  : The answer is open fridge, pick drink, close fridge, bring drink\n"
     ]
    }
   ],
   "source": [
    "# quick smoke test for templates\n",
    "\n",
    "REAL_EXAMPLES = {\n",
    "    \"csqa\": {\n",
    "        \"question\": \"Where would you find a hamburger?\",\n",
    "        \"choices\": {\n",
    "            \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "            \"text\": [\"fast food restaurant\", \"pizza\", \"kitchen\", \"grocery store\", \"farms\"]\n",
    "        },\n",
    "        \"answerKey\": \"A\"\n",
    "    },\n",
    "    \"strategyqa\": {\n",
    "        \"question\": \"Would a person with a high fever typically sweat?\",\n",
    "        \"answer\": True\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"input\": \"What day of the week was 2020-02-02?\",\n",
    "        \"target_scores\": {\"Sunday\": 1, \"Monday\": 0, \"Tuesday\": 0}\n",
    "    },\n",
    "    \"sports\": {\n",
    "        \"input\": \"The quarterback threw a touchdown pass.\",\n",
    "        \"target_scores\": {\"plausible\": 1}\n",
    "    },\n",
    "    \"saycan\": {\n",
    "        \"instruction\": \"Bring me a cold drink.\",\n",
    "        \"plan\": \"open fridge, pick drink, close fridge, bring drink\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def test_smoke():\n",
    "    \"\"\"Sanity check with the hand-crafted examples above.\"\"\"\n",
    "    templates = {\n",
    "        \"csqa\": csqa_template,\n",
    "        \"strategyqa\": strategyqa_template,\n",
    "        \"date\": date_template,\n",
    "        \"sports\": sports_template,\n",
    "        \"saycan\": saycan_template,\n",
    "    }\n",
    "\n",
    "    for dataset, tpl in templates.items():\n",
    "        print(f\"\\n--- {dataset} ---\")\n",
    "        q, a = tpl(REAL_EXAMPLES[dataset])\n",
    "        print(\"Question:\", q)\n",
    "        print(\"Answer  :\", a)\n",
    "        # basic assertions\n",
    "        assert isinstance(q, str) and q.strip()\n",
    "        assert isinstance(a, str) and a.strip()\n",
    "\n",
    "test_smoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c523bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is the following sentence plausible? \"Jamal Murray was perfect from the line\"',\n",
       " 'The answer is yes')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_template(sports['examples'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for CSQA\n",
    "\n",
    "CSQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\n",
    "calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.\n",
    "So the answer is (e).\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
    "A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\n",
    "Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b)\n",
    "hen house (c) natural habitat (d) storybook\n",
    "A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the\n",
    "answer is (b).\n",
    "Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\n",
    "(b) race track (c) desert (d) apartment (e) roadblock\n",
    "A: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of\n",
    "people. So the answer is (a).\n",
    "Q: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\n",
    "market (d) fruit basket (e) fruit market\n",
    "A: The answer should be the place where grocery items are placed before checking out. Of the above choices,\n",
    "grocery cart makes the most sense for holding grocery items. So the answer is (b).\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A: The answer must be something that used to do what Google Maps and GPS services do, which is to give\n",
    "directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\n",
    "Q: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\n",
    "anguish (c) bitterness (d) tears (e) sadness\n",
    "A: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above\n",
    "choices, the closest feeling is bitterness. So the answer is (c).\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b798037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for StrategyQA\n",
    "\n",
    "StrategyQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: Do hamsters provide food for any animals?\n",
    "A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So\n",
    "the answer is yes.\n",
    "Q: Could Brooke Shields succeed at University of Pennsylvania?\n",
    "A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\n",
    "University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the\n",
    "answer is yes.\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic\n",
    "number squared is less than 5. So the answer is no.\n",
    "Q: Yes or no: Is it common to see frost during some college commencements?\n",
    "A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\n",
    "there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\n",
    "Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n",
    "A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\n",
    "months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\n",
    "Q: Yes or no: Would a pear sink in water?\n",
    "A: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus,\n",
    "a pear would float. So the answer is no.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for date understanding \n",
    "\n",
    "Date_PROMPT = \"\"\"\n",
    "\n",
    "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today\n",
    "is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.\n",
    "Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in\n",
    "MM/DD/YYYY?\n",
    "A: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first monday, would be six\n",
    "days later. So today is 01/07/2019. So the answer is 01/07/2019.\n",
    "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\n",
    "days ago in MM/DD/YYYY?\n",
    "A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the\n",
    "answer is 05/23/1943.\n",
    "Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is\n",
    "04/20/1969.\n",
    "Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\n",
    "later in MM/DD/YYYY?\n",
    "A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\n",
    "Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\n",
    "yesterday in MM/DD/YYYY?\n",
    "A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so\n",
    "today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ebddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for sports understanding\n",
    "\n",
    "Sports_PROMPT = \"\"\"\n",
    "\n",
    "Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\n",
    "A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\n",
    "A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”\n",
    "A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer\n",
    "is no.\n",
    "Q: Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\n",
    "A: Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Jamel Murray was perfect from the line.”\n",
    "A: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Sam Darnold passed the puck.”\n",
    "A: Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A: Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\n",
    "A: Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. So the answer is yes.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c971580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for saycan \n",
    "\n",
    "SayCan_PROMPT = \"\"\"\n",
    "\n",
    "Locations = [counter, table, user, trash, bowl].\n",
    "Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\n",
    "grapefruit soda, pepsi, redbull, energy bar, sponge, water].\n",
    "The robot can pick up items with pick(object) and put down items with put(object) as well as find objects or\n",
    "locations with find(). The robot can only understand the explicit locations and objects listed.\n",
    "Human: I’m hungry, can you bring me some chips.\n",
    "Explanation: The user is hungry and has asked for chips. There are several types of chips available, I will bring\n",
    "the user the kettle chips.\n",
    "Plan: 1. find(kettle chips), 2. pick(kettle chips), 3. find(user), 4. put(kettle chips), 5. done().\n",
    "Human: How would you move the grapefruit drink from the table to the counter?\n",
    "Explanation: The user has asked me to move the grapefruit drink to the counter.\n",
    "Plan: 1. find(grapefruit soda), 2. pick(grapefruit soda), 3. find(counter), 4. put(grapefruit soda), 5. done().\n",
    "Human: How would you bring me some snacks?\n",
    "Explanation: The user has asked for snacks, I will choose two items and bring them. I will bring jalapeno chips\n",
    "and an apple.\n",
    "Plan: 1. find(jalapeno chips), 2. pick(jalapeno chips), 3. find(user), 4. put(jalapeno chips), 5. find(apple), 6.\n",
    "pick(apple), 7. find(user), 8. put(apple), 9. done().\n",
    "Human: How would you bring me something to eat that isn’t a fruit?\n",
    "Explanation: The user has asked for a food that isn’t a fruit, I will bring an energy bar to them.\n",
    "Plan: 1. find(energy bar), 2. pick(energy bar), 3. find(user), 4. put(energy bar), 5. done().\n",
    "Human: How would you put the rice chips in the bowl and then move the tea to the table?\n",
    "Explanation: The user has asked me to do two tasks, I will do one and then the other.\n",
    "Plan: 1. find(rice chips), 2. pick(rice chips), 3. find(bowl), 4. put(rice chips), 5. find(tea), 6. pick(tea), 7.\n",
    "find(table), 8. put(tea), 9. done().\n",
    "Human: How would you throw away a redbull?\n",
    "Explanation: The user has asked me to throw away the redbull, I will move it to the trash.\n",
    "Plan: 1. find(redbull), 2. pick(redbull), 3. find(trash), 4. put(redbull), 5. done().\n",
    "Human: Bring me a drink.\n",
    "Explanation: The user has asked for a drink and there are many options. I will bring them a water.\n",
    "Plan: 1. find(water), 2. pick(water), 3. find(user), 4. put(water), 5. done().\n",
    "\n",
    "\n",
    "Human: {Question}\n",
    "Explanation:\n",
    "Plan:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d3e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
