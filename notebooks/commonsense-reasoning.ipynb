{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc318f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thyag/Desktop/COT/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from openai import OpenAI\n",
    "from google.genai import types\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# platform agnostic method to load API keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv('GEMINI_API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd8c4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m strategyqa = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mmetaeval/strategy-qa\u001b[39m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# train / test splits\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Fields (among others): question, answer (bool), facts, decomposition\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 3. Date – CLOSURE-style date inference (from SuperGLUE)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m date =  \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/date_understanding/task.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# “wsc” task contains date-relevant\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# If you want the pure “Date” subset used in some papers, filter\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# e.g. date = date.filter(lambda x: x[\"label\"] in [\"before\", \"after\", \"same\"])\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 4. Sports – Sentence-plausibility judgment (also from SuperGLUE)\u001b[39;00m\n\u001b[32m     22\u001b[39m sports = pd.read_json(\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/sports_understanding/task.json\u001b[39m\u001b[33m\"\u001b[39m)     \u001b[38;5;66;03m# same loader as Date\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1014\u001b[39m, in \u001b[36mJsonReader.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1012\u001b[39m         obj = \u001b[38;5;28mself\u001b[39m._get_object_parser(\u001b[38;5;28mself\u001b[39m._combine_lines(data_lines))\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.convert_dtypes(\n\u001b[32m   1017\u001b[39m         infer_objects=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend=\u001b[38;5;28mself\u001b[39m.dtype_backend\n\u001b[32m   1018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1040\u001b[39m, in \u001b[36mJsonReader._get_object_parser\u001b[39m\u001b[34m(self, json)\u001b[39m\n\u001b[32m   1038\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mframe\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     obj = \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1176\u001b[39m, in \u001b[36mParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1174\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COT/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1392\u001b[39m, in \u001b[36mFrameParser._parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1388\u001b[39m orient = \u001b[38;5;28mself\u001b[39m.orient\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1391\u001b[39m     \u001b[38;5;28mself\u001b[39m.obj = DataFrame(\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1393\u001b[39m     )\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1395\u001b[39m     decoded = {\n\u001b[32m   1396\u001b[39m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float=\u001b[38;5;28mself\u001b[39m.precise_float).items()\n\u001b[32m   1398\u001b[39m     }\n",
      "\u001b[31mValueError\u001b[39m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import os, json, requests, gzip\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "# 1. CSQA – CommonsenseQA (train/dev/test splits)\n",
    "csqa = load_dataset(\"commonsense_qa\")          # returns DatasetDict\n",
    "# Fields: id, question, question_concept, choices:{label,text}, answerKey\n",
    "\n",
    "\n",
    "# 2. StrategyQA – yes/no questions requiring multi-hop reasoning\n",
    "strategyqa = load_dataset(\"metaeval/strategy-qa\")   # train / test splits\n",
    "# Fields (among others): question, answer (bool), facts, decomposition\n",
    "\n",
    "\n",
    "# 5. SayCan – Language→robot-action mapping (Google Robotics, 2022)\n",
    "saycan = load_dataset(\"chiayewken/saycan\")\n",
    "# Each record: {\"instruction\": str, \"plan\": [{\"type\":\"pick\", \"object\":\"apple\"}, ...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is the following sentence plausible? \"Jamal Murray was perfect from the line\"'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "sports_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/sports_understanding/task.json\"\n",
    "sports_response = requests.get(sports_url)\n",
    "sports = json.loads(sports_response.text)\n",
    "\n",
    "def sports_example_to_question(example):\n",
    "    \"\"\"\n",
    "    Converts a sports example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'input' and 'target_scores'.\n",
    "    Returns:\n",
    "        str: Formatted question string.\n",
    "    \"\"\"\n",
    "    return f'Is the following sentence plausible? \"{example[\"input\"]}\"'\n",
    "\n",
    "sports_example_to_question(sports['examples'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63f8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Yesterday was April 30, 2021. What is the date today in MM/DD/YYYY?',\n",
      " 'target_scores': {'02/23/2021': 0,\n",
      "                   '03/11/2021': 0,\n",
      "                   '04/29/2021': 0,\n",
      "                   '05/01/2021': 1,\n",
      "                   '05/09/2021': 0,\n",
      "                   '06/12/2021': 0}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "date_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\n",
    "date_response = requests.get(date_url)\n",
    "date = json.loads(date_response.text)\n",
    "pprint(date['examples'][0])\n",
    "\n",
    "def date_example_to_question(example):\n",
    "    \"\"\"\n",
    "    Converts a date example dict to the question template.\n",
    "    Args:\n",
    "        example (dict): Dictionary with 'input' and 'target_scores'.\n",
    "    Returns:\n",
    "        str: Formatted question string.\n",
    "    \"\"\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53aec3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSQA: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "StrategyQA: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['qid', 'term', 'description', 'question', 'answer', 'facts', 'decomposition'],\n",
      "        num_rows: 2290\n",
      "    })\n",
      "})\n",
      "Date: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 146\n",
      "    })\n",
      "})\n",
      "Sports: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'span1_index', 'span2_index', 'span1_text', 'span2_text', 'idx', 'label'],\n",
      "        num_rows: 146\n",
      "    })\n",
      "})\n",
      "SayCan: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['INPUT', 'OUTPUT'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity checks\n",
    "for name, ds in [(\"CSQA\", csqa), (\"StrategyQA\", strategyqa),\n",
    "                 (\"Date\", date), (\"Sports\", sports), (\"SayCan\", saycan)]:\n",
    "    print(f\"{name}: {len(ds) if isinstance(ds, list) else ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b31b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE_QUESTION sample: Emma did not pass the ball to Janie although she was open. \n",
      "\n",
      "SPORTS_QUESTION sample: Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical. \n",
      "\n",
      "SAYCAN_QUESTION sample: Can you bring all the snacks on the table? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATE_QUESTION = date['train']['text']\n",
    "print(f\"DATE_QUESTION sample: {DATE_QUESTION[10]} \\n\")  \n",
    "SPORTS_QUESTION = sports['train']['text']\n",
    "print(f\"SPORTS_QUESTION sample: {SPORTS_QUESTION[0]} \\n\")\n",
    "SAYCAN_QUESTION = saycan['test']['INPUT']\n",
    "print(f\"SAYCAN_QUESTION sample: {SAYCAN_QUESTION[0]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931530a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8c173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Answer Choices: (a) ignore (b) enforce (c) authoritarian (d) yell at (e) avoid'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def csqa_question_template(question: str, choices: dict) -> str:\n",
    "    \"\"\"\n",
    "    Formats a CSQA question with answer choices in the required template.\n",
    "    Args:\n",
    "        question (str): The question text.\n",
    "        choices (dict): Dictionary with 'label' and 'text' keys.\n",
    "    Returns:\n",
    "        str: Formatted question string.\n",
    "    \"\"\"\n",
    "    # Map labels to lowercase letters (a, b, c, ...)\n",
    "    labels = [label.lower() for label in choices['label']]\n",
    "    texts = choices['text']\n",
    "    # Combine choices into \"(a) choice1 (b) choice2 ...\"\n",
    "    choices_str = \" \".join([f\"({label}) {text}\" for label, text in zip(labels, texts)])\n",
    "    return f\"{question} Answer Choices: {choices_str}\"\n",
    "\n",
    "csqa_question_template(csqa['train']['question'][0], csqa['train']['choices'][0])\n",
    "\n",
    "def strategyqa_question_template(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats a StrategyQA question in the required template.\n",
    "    Args:\n",
    "        question (str): The question text.\n",
    "    Returns:\n",
    "        str: Formatted question string.\n",
    "    \"\"\"\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for CSQA\n",
    "\n",
    "CSQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\n",
    "calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.\n",
    "So the answer is (e).\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
    "A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\n",
    "Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b)\n",
    "hen house (c) natural habitat (d) storybook\n",
    "A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the\n",
    "answer is (b).\n",
    "Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\n",
    "(b) race track (c) desert (d) apartment (e) roadblock\n",
    "A: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of\n",
    "people. So the answer is (a).\n",
    "Q: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\n",
    "market (d) fruit basket (e) fruit market\n",
    "A: The answer should be the place where grocery items are placed before checking out. Of the above choices,\n",
    "grocery cart makes the most sense for holding grocery items. So the answer is (b).\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A: The answer must be something that used to do what Google Maps and GPS services do, which is to give\n",
    "directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\n",
    "Q: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\n",
    "anguish (c) bitterness (d) tears (e) sadness\n",
    "A: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above\n",
    "choices, the closest feeling is bitterness. So the answer is (c).\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b798037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for StrategyQA\n",
    "\n",
    "StrategyQA_PROMPT = \"\"\"\n",
    "\n",
    "Q: Do hamsters provide food for any animals?\n",
    "A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So\n",
    "the answer is yes.\n",
    "Q: Could Brooke Shields succeed at University of Pennsylvania?\n",
    "A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the\n",
    "University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the\n",
    "answer is yes.\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic\n",
    "number squared is less than 5. So the answer is no.\n",
    "Q: Yes or no: Is it common to see frost during some college commencements?\n",
    "A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\n",
    "there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\n",
    "Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n",
    "A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\n",
    "months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\n",
    "Q: Yes or no: Would a pear sink in water?\n",
    "A: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus,\n",
    "a pear would float. So the answer is no.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for date understanding \n",
    "\n",
    "Date_PROMPT = \"\"\"\n",
    "\n",
    "Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today\n",
    "is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.\n",
    "Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in\n",
    "MM/DD/YYYY?\n",
    "A: If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first monday, would be six\n",
    "days later. So today is 01/07/2019. So the answer is 01/07/2019.\n",
    "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\n",
    "days ago in MM/DD/YYYY?\n",
    "A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the\n",
    "answer is 05/23/1943.\n",
    "Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is\n",
    "04/20/1969.\n",
    "Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\n",
    "later in MM/DD/YYYY?\n",
    "A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.\n",
    "Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\n",
    "yesterday in MM/DD/YYYY?\n",
    "A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so\n",
    "today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ebddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for sports understanding\n",
    "\n",
    "Sports_PROMPT = \"\"\"\n",
    "\n",
    "Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\n",
    "A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\n",
    "A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”\n",
    "A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer\n",
    "is no.\n",
    "Q: Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\n",
    "A: Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Jamel Murray was perfect from the line.”\n",
    "A: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. So the answer is yes.\n",
    "Q: Is the following sentence plausible? “Sam Darnold passed the puck.”\n",
    "A: Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A: Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. So the\n",
    "answer is no.\n",
    "Q: Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\n",
    "A: Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. So the answer is yes.\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c971580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for saycan \n",
    "\n",
    "SayCan_PROMPT = \"\"\"\n",
    "\n",
    "Locations = [counter, table, user, trash, bowl].\n",
    "Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\n",
    "grapefruit soda, pepsi, redbull, energy bar, sponge, water].\n",
    "The robot can pick up items with pick(object) and put down items with put(object) as well as find objects or\n",
    "locations with find(). The robot can only understand the explicit locations and objects listed.\n",
    "Human: I’m hungry, can you bring me some chips.\n",
    "Explanation: The user is hungry and has asked for chips. There are several types of chips available, I will bring\n",
    "the user the kettle chips.\n",
    "Plan: 1. find(kettle chips), 2. pick(kettle chips), 3. find(user), 4. put(kettle chips), 5. done().\n",
    "Human: How would you move the grapefruit drink from the table to the counter?\n",
    "Explanation: The user has asked me to move the grapefruit drink to the counter.\n",
    "Plan: 1. find(grapefruit soda), 2. pick(grapefruit soda), 3. find(counter), 4. put(grapefruit soda), 5. done().\n",
    "Human: How would you bring me some snacks?\n",
    "Explanation: The user has asked for snacks, I will choose two items and bring them. I will bring jalapeno chips\n",
    "and an apple.\n",
    "Plan: 1. find(jalapeno chips), 2. pick(jalapeno chips), 3. find(user), 4. put(jalapeno chips), 5. find(apple), 6.\n",
    "pick(apple), 7. find(user), 8. put(apple), 9. done().\n",
    "Human: How would you bring me something to eat that isn’t a fruit?\n",
    "Explanation: The user has asked for a food that isn’t a fruit, I will bring an energy bar to them.\n",
    "Plan: 1. find(energy bar), 2. pick(energy bar), 3. find(user), 4. put(energy bar), 5. done().\n",
    "Human: How would you put the rice chips in the bowl and then move the tea to the table?\n",
    "Explanation: The user has asked me to do two tasks, I will do one and then the other.\n",
    "Plan: 1. find(rice chips), 2. pick(rice chips), 3. find(bowl), 4. put(rice chips), 5. find(tea), 6. pick(tea), 7.\n",
    "find(table), 8. put(tea), 9. done().\n",
    "Human: How would you throw away a redbull?\n",
    "Explanation: The user has asked me to throw away the redbull, I will move it to the trash.\n",
    "Plan: 1. find(redbull), 2. pick(redbull), 3. find(trash), 4. put(redbull), 5. done().\n",
    "Human: Bring me a drink.\n",
    "Explanation: The user has asked for a drink and there are many options. I will bring them a water.\n",
    "Plan: 1. find(water), 2. pick(water), 3. find(user), 4. put(water), 5. done().\n",
    "\n",
    "\n",
    "Q: {Question}\n",
    "A:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d3e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
