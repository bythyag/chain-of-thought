# replication experiment on chain-of-thought reasoning.

main paper link: [chain-of-thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903)

## paper summary.

### introduction. 
llms are capable of a multitude of language understanding tasks such as question answering, analysis, translation, and information extraction. however, problems in logical reasoning and arithmetic require multiple reasoning steps to solve correctly. for such problem categories, models like gpt-3 and palm exhibit low accuracy scores. while these problems can be addressed through fine-tuning for specific tasks, creating datasets for such cases is tedious, costly, and time-consuming.

to address this, the paper proposes solving these problems using prompting methods. it introduces chain of thought (cot) prompting, which improves poor reasoning performance in language models by:

- generating a series of intermediate reasoning steps before arriving at the final solution,
- using few-shot examples that include reasoning steps to guide the model,
- formatting prompts as triples of `<input, chain of thought, output>` for reference.

cot prompting demonstrates significant performance improvements across arithmetic, commonsense, and symbolic reasoning tasks.

### why does it work?
cot prompting enables the model to decompose multi-step problems into intermediate steps. it also provides an interpretable reasoning path, which is useful for understanding the model’s logic and debugging incorrect answers.

### example prompt.
user message:
```python
question 1:
roger has 5 tennis balls. he buys 2 more cans of tennis balls. each can has 3 tennis balls.
how many tennis balls does he have now?

answer:
roger started with 5 balls.
2 cans × 3 tennis balls = 6 balls.
5 + 6 = 11
the answer is 11.

question 2:
the cafeteria had 23 apples. they used 20 to make lunch and bought 6 more.
how many apples do they have now?
```

llm response:
```bash
the cafeteria had 23 apples originally.
they used 20: 23 − 20 = 3 apples left.
they bought 6 more: 3 + 6 = 9
the answer is 9.
```

### limitations.
1. the cot prompting emulates the thought process of the annotator and does not imply directly if the model is actually reasoning.
2. cost of annotation is comparatively low for few-shot prompting but it can increase considerably if we want to fine-tune the model.
3. there is no guarantee of correct reasoning path generated by the model and hence can lead to both correct and incorrect answers.
4. improving factual generations of language models is an open direction for future work.
cot reasoning only improves the performance of larger models making it costly to serve in real-world applications.

### conclusion.
1. cot improves performance by a significant margin on arithmetic reasoning.
2. the corresponding improvements are stronger than ablation and robust to annotations.
3. experiments on commonsense reasoning of the models also show considerable improvements.
4. in symbolic reasoning, cot can help solve longer problem sequences compared to the few-shot examples.

## experiment results.

_work in progress_

## key insights.

_work in progress_

## references.

_work in progress_

## contribution.

in collaboration with [@twofifteenam](https://github.com/teltam)

## citation.

if you incorporate our results or code in your experiment, please cite our work as follows.

```bibtex
@software{raj2025cot,
  author       = {raj, thyag and twofifteenam, teltam},
  title        = {replication experiment on chain-of-thought reasoning},
  month        = may,
  year         = 2025,
  publisher    = {github},
  version      = {1.0.0},
  url          = {https://github.com/bythyag/chain-of-thought},
  abstract     = {replication study of chain-of-thought prompting techniques for improving reasoning capabilities in large language models.}
}
```

you can also cite this work using the following format:

raj, t., & twofifteenam, t. (2025). replication experiment on chain-of-thought reasoning (version 1.0.0) [software]. https://github.com/bythyag/chain-of-thought
